{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Distributed Model Training\n",
    "\n",
    "### University of Virginia\n",
    "### DS 7200: Distributed Computing\n",
    "### Last Updated: June 5, 2024\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### SOURCES: \n",
    "\n",
    "- [Distributed Machine Learning Frameworks and its Benefits](https://www.xenonstack.com/blog/distributed-ml-framework#:~:text=In%20distributed%20machine%20learning%2C%20model,and%20training%20each%20split%20separately.)\n",
    "\n",
    "- [Distributed Training with Azure](https://learn.microsoft.com/en-us/azure/machine-learning/concept-distributed-training?view=azureml-api-2/)\n",
    "\n",
    "- [Distributed Training: Guide for Data Scientists\n",
    "](https://neptune.ai/blog/distributed-training)\n",
    "\n",
    "Need to Review:\n",
    "\n",
    "- [Distributed model training II: Parameter Server and AllReduce](http://www.juyang.co/distributed-model-training-ii-parameter-server-and-allreduce/)\n",
    "\n",
    "- [Distributed Machine Learning and the Parameter Server](https://www.cs.cornell.edu/courses/cs4787/2019sp/notes/lecture22.pdf)\n",
    "\n",
    "\n",
    "\n",
    "### OBJECTIVES\n",
    "\n",
    "- Explain uses \n",
    "\n",
    "### CONCEPTS\n",
    "\n",
    "- Data parallelism and model parallelism\n",
    "- Asynchronous training vs Synchronous training \n",
    "- Parameter server algorithm\n",
    "- All-reduce algorithm\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### I. Why use Distributed Model Training?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For massive training sets, it may not be possible to train a model on a single machine. \n",
    "\n",
    "This may be the case for deep learning models.\n",
    "\n",
    "In *distributed training*, the workload to train a model is split up and shared among worker nodes. \n",
    "\n",
    "The concepts, benefits, and challenges are similar to what we've learned earlier.\n",
    "\n",
    "The work can be parallelized to speed up training.\n",
    "\n",
    "This process introduces benefits but also complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### II. Data Parallelism and Model Parallelism\n",
    "\n",
    "Two main types of distributed training: *data parallelism* and *model parallelism*.\n",
    "\n",
    "#### Data Parallelism\n",
    "\n",
    "This follows the approach used by Spark\n",
    "\n",
    "Data is divided into partitions\n",
    "\n",
    "Number of partitions = total number of available nodes\n",
    "\n",
    "Model is copied in each worker node\n",
    "\n",
    "Each node operates on its subset of data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./data_parallelism.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each node must:\n",
    "\n",
    "- Independently compute errors between training sample predictions and labels\n",
    "- Update its model based on errors\n",
    "- Communicate all of its changes to the other nodes to update their corresponding models\n",
    "\n",
    "Worker nodes need to synchronize gradients at end of batch computation to ensure they're training a consistent model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Parallelism\n",
    "\n",
    "Model is segmented into different parts that run concurrently in different nodes\n",
    "\n",
    "Each model part runs on same data\n",
    "\n",
    "Scalability depends on degree of task parallelization of algorithm\n",
    "\n",
    "Worker nodes need to synchronize shared parameters, usually once for each forward or backward-propagation step. \n",
    "\n",
    "More complex to implement than data parallelism"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### III. Synchronous Training\n",
    "\n",
    "Consider data parallelism case: \n",
    "- Data is divided into partitions\n",
    "- Each partition is sent to a worker \n",
    "- Each worker has full replica of model. Training is done on its partition. \n",
    "\n",
    "**Forward Pass**\n",
    "\n",
    "In synchronous training, forward pass begins at same time for each worker\n",
    "\n",
    "Each worker computes different output and gradients\n",
    "\n",
    "Each worker waits for the others to complete training loops and calculate respective gradients\n",
    "\n",
    "After all workers have computed gradients, they communicate with each other and aggregate gradients using *all-reduce algorithm* (below)\n",
    "\n",
    "After all gradients are combined, the updated gradients are copied to all workers. \n",
    "\n",
    "**Backward Pass**\n",
    "\n",
    "Each worker performs backward pass and updates their local weights\n",
    "\n",
    "Each worker will have different gradients as they are trained on different subsets of data\n",
    "\n",
    "However, at any point in time, all workers have the same weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./cosine_sim.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synchronous Algorithm Example: : All-Reduce Algorithm\n",
    "\n",
    "AllReduce: To synchronize the model weights across all computers in a cluster, the AllReduce method is utilized. Using a portion of the training data, each computer calculates the model's gradient and distributes it to the other machines. The gradients are then combined using the AllReduce method, which also updates the model weights on each computer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./cosine_sim.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### IV. Asynchronous Training\n",
    "\n",
    "Asynchronous training can be more efficient than synchronous training since there is no waiting. \n",
    "\n",
    "This is especially helpful when there is variation in the computing power across workers.\n",
    "\n",
    "Thus in asynchronous training, we want workers to work independently in such a way that a worker need not wait for any other worker in the cluster. One way to achieve this is by using a parameter server."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./vector_space.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Asynchronous Algorithm Example: Parameter Server\n",
    "\n",
    "In this approach, the weights and biases of the ML model are distributed across nodes in the cluster. \n",
    "\n",
    "A copy of the model is stored on each node and a centralized *parameter server* manages model changes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./parameter_server.png\" width=300>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### V. Challenges\n",
    "\n",
    "Ensuring the convergence of the model during distributed training is a significant difficulty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./rag.png\" width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementation\n",
    "\n",
    "**Elephas** is a Keras add-on that allows you to use Spark to execute distributed deep learning models at scale.\n",
    "\n",
    "**Amazon SageMaker** offers \n",
    "\n",
    "**Horovod**. Open-source distributed training framework developed at Uber for TensorFlow, Keras, PyTorch, and MXNet.  \n",
    "Supports practical distributed training over several GPUs and nodes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusions\n",
    "\n",
    "Add details\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
